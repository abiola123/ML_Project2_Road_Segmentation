{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ce8841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import tensorflow as tf\n",
    "import os\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import code\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 3  # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16  # 64\n",
    "NUM_EPOCHS = 100\n",
    "RESTORE_MODEL = True  # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 0\n",
    "IMG_PATCH_SIZE = 16\n",
    "root_dir = \"/Users/abiola/Documents/EPFL/ML_Project2_Road_Segmentation/Ressources/training/\"\n",
    "\n",
    "# Extract patches from a given image\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    image_dir = root_dir + \"out/images/\"\n",
    "    files = os.listdir(image_dir)\n",
    "    n = num_images\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    imgs = [load_image(image_dir + files[i])[:,:,:3] for i in range(n)]\n",
    "    imgs = np.asarray(imgs)\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    print(\"NB patches / imgae: \", N_PATCHES_PER_IMAGE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)\n",
    "\n",
    "\n",
    "\n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25  # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:  # road\n",
    "        return [0, 1]\n",
    "    else:  # bgrd\n",
    "        return [1, 0]\n",
    "\n",
    "\n",
    "    # Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    gt_dir = root_dir + \"out/groundtruth/\"\n",
    "    files = os.listdir(gt_dir)\n",
    "\n",
    "    n = num_images\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    gt_imgs = [load_image(gt_dir + files[i])[:,:,0] for i in range(n)]\n",
    "    gt_imgs = np.asarray(gt_imgs)\n",
    "    \n",
    "    print(gt_imgs[0])\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and 1-hot labels.\"\"\"\n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1)) /\n",
    "        predictions.shape[0])\n",
    "\n",
    "# Write predictions from neural network to a file\n",
    "def write_predictions_to_file(predictions, labels, filename):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    file = open(filename, \"w\")\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(0, n):\n",
    "        file.write(max_labels(i) + ' ' + max_predictions(i))\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    \n",
    "# Print predictions from neural network\n",
    "def print_predictions(predictions, labels):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    print(str(max_labels) + ' ' + str(max_predictions))\n",
    "\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if labels[idx][0] > 0.5:  # bgrd\n",
    "                l = 0\n",
    "            else:\n",
    "                l = 1\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    n_channels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if n_channels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:, :, 0] = gt_img8\n",
    "        gt_img_3c[:, :, 1] = gt_img8\n",
    "        gt_img_3c[:, :, 2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "    color_mask[:, :, 0] = predicted_img*PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "044c1aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 images\n",
      "NB patches / imgae:  625.0\n",
      "Loading 20 images\n",
      "[[0.07843138 0.07843138 0.07843138 ... 0.07843138 0.07843138 0.07843138]\n",
      " [0.07843138 0.07843138 0.07843138 ... 0.07843138 0.07843138 0.07843138]\n",
      " [0.07843138 0.07843138 0.07843138 ... 0.07843138 0.07843138 0.07843138]\n",
      " ...\n",
      " [0.07843138 0.07843138 0.07843138 ... 0.07843138 0.07843138 0.07843138]\n",
      " [0.07843138 0.07843138 0.07843138 ... 0.07843138 0.07843138 0.07843138]\n",
      " [0.07843138 0.07843138 0.07843138 ... 0.07843138 0.07843138 0.07843138]]\n",
      "(12500, 2)\n",
      "(12500, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract it into numpy arrays.\n",
    "\n",
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "X = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "y = extract_labels(train_labels_filename, TRAINING_SIZE)\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e23c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 3046 c1 = 3046\n",
      "Balancing training data...\n"
     ]
    }
   ],
   "source": [
    "# Blance training data \n",
    "c0 = 0  # bgrd\n",
    "c1 = 0  # road\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "print('Balancing training data...')\n",
    "\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(y) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(y) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "\n",
    "\n",
    "X = X[new_indices, :, :, :]\n",
    "y = y[new_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8638a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metric=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47690810",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_17 to have shape (1,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cf51fa18c55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             first_x_value, batch_size, self._distribution_strategy)\n\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    989\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0m\u001b[1;32m    992\u001b[0m                                                      class_weight, batch_size)\n\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m       \u001b[0;31m# Standardize the outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m       y = training_utils.standardize_input_data(\n\u001b[0m\u001b[1;32m   1150\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0mfeed_output_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mref_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mref_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    330\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_17 to have shape (1,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "model.fit(X,y, batch_size = BATCH_SIZE, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695055f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d51ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
